<!doctype html>
<html lang="hi">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>üîÆ AstroOne Live ‚Äî Sumit Aggarwal</title>
<style>
  :root{--accent:#6a11cb;--accent-dark:#4a0fa8}
  body{margin:0;height:100vh;display:flex;align-items:center;justify-content:center;background:linear-gradient(135deg,#6a11cb,#2575fc);font-family:Inter,system-ui,Arial;color:#222}
  .card{width:480px;background:#fff;border-radius:14px;padding:22px;box-shadow:0 14px 40px rgba(0,0,0,0.15)}
  h1{margin:0 0 10px;text-align:center;font-size:20px}
  label{font-size:13px;color:#444;margin-top:8px;display:block}
  input,select,button{width:100%;padding:10px;margin-top:6px;border-radius:8px;border:1px solid #ddd;font-size:14px;box-sizing:border-box}
  .row{display:flex;gap:10px}
  #connect{background:var(--accent);color:#fff;border:none;cursor:pointer}
  #connect:hover{background:var(--accent-dark)}
  #disconnect{background:#e74c3c;color:#fff;border:none;cursor:pointer;display:none}
  #status{margin-top:12px;color:#333;min-height:20px;text-align:center}
  .muted{font-size:12px;color:#888;text-align:center;margin-top:8px}
</style>
</head>
<body>
  <div class="card">
    <h1>üîÆ AstroOne Live ‚Äî Talk with Sumit Aggarwal</h1>

    <label>‡§™‡•Ç‡§∞‡§æ ‡§®‡§æ‡§Æ</label>
    <input id="name" placeholder="‡§™‡•Ç‡§∞‡§æ ‡§®‡§æ‡§Æ">

    <div class="row">
      <div style="flex:1">
        <label>DOB</label>
        <input id="dob" type="date">
      </div>
      <div style="flex:1">
        <label>Time</label>
        <input id="tob" type="time">
      </div>
    </div>

    <label>Place of Birth (POB)</label>
    <input id="pob" placeholder="‡§∂‡§π‡§∞, ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§Ø‡§æ ‡§¶‡•á‡§∂">

    <div class="row">
      <div style="flex:1">
        <label>Gender</label>
        <select id="gender"><option value="">Select</option><option value="male">Male</option><option value="female">Female</option></select>
      </div>
      <div style="flex:1">
        <label>Voice</label>
        <select id="voice">
          <option value="verse">Verse</option>
          <option value="alloy">Alloy</option>
          <option value="ash">Ash</option>
          <option value="ballad">Ballad</option>
          <option value="coral">Coral</option>
          <option value="echo">Echo</option>
          <option value="sage">Sage</option>
          <option value="shimmer">Shimmer</option>
          <option value="marin">Marin</option>
          <option value="cedar">Cedar</option>
        </select>
      </div>
    </div>

    <div style="margin-top:12px;display:flex;gap:10px">
      <button id="connect">‚ñ∂Ô∏è Start Live Call</button>
      <button id="disconnect">‚èπ Stop</button>
    </div>

    <div id="status">Fill details and press Start.</div>
    <div class="muted">Note: Allow microphone permissions in your browser.</div>
  </div>

<script>
/*
  client/index_ws.html
  - Streams mic PCM->base64 to server
  - Sends periodic commits to trigger OpenAI response
  - Receives base64 raw PCM frames from server and plays them
  - Automatically resamples source PCM (assumed 16000 Hz) to audio output sample rate
*/

const connectBtn = document.getElementById("connect");
const disconnectBtn = document.getElementById("disconnect");
const statusEl = document.getElementById("status");

let ws = null;
let audioCtx = null;
let micStream = null;
let mediaSource = null;
let scriptNode = null;
let commitTimer = null;

// Helper: set status
function setStatus(text) { statusEl.innerText = text; console.log(text); }

// Convert Float32Array -> 16-bit PCM LE -> base64
function floatToBase64(float32Array) {
  const len = float32Array.length;
  const buffer = new ArrayBuffer(len * 2);
  const view = new DataView(buffer);
  for (let i = 0; i < len; i++) {
    let s = Math.max(-1, Math.min(1, float32Array[i]));
    view.setInt16(i * 2, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
  }
  // Convert to binary string in chunks and base64
  let binary = "";
  const bytes = new Uint8Array(buffer);
  const CHUNK = 0x8000;
  for (let i = 0; i < bytes.length; i += CHUNK) {
    binary += String.fromCharCode.apply(null, bytes.subarray(i, i + CHUNK));
  }
  return btoa(binary);
}

// Play base64 raw PCM (assumed sampleRate = 16000 by default).
// If browser output sampleRate != sourceRate, resample using OfflineAudioContext.
async function playPCMBase64(b64, sourceSampleRate = 16000) {
  try {
    const bytes = Uint8Array.from(atob(b64), c => c.charCodeAt(0));
    const frameCount = bytes.length / 2;
    const float32 = new Float32Array(frameCount);
    for (let i = 0; i < frameCount; i++) {
      const lo = bytes[i * 2];
      const hi = bytes[i * 2 + 1];
      const val = (hi << 8) | lo;
      const signed = val > 32767 ? val - 65536 : val;
      float32[i] = signed / 32768;
    }

    // Create AudioBuffer with sourceSampleRate
    if (!audioCtx) audioCtx = new (window.AudioContext || window.webkitAudioContext)();
    const outputSampleRate = audioCtx.sampleRate || 48000;

    // If same sample rate, play directly
    if (Math.abs(outputSampleRate - sourceSampleRate) < 1) {
      const buffer = audioCtx.createBuffer(1, frameCount, sourceSampleRate);
      buffer.getChannelData(0).set(float32);
      const src = audioCtx.createBufferSource();
      src.buffer = buffer;
      src.connect(audioCtx.destination);
      src.start();
      return;
    }

    // Resample using OfflineAudioContext
    const offlineCtx = new OfflineAudioContext(1, Math.ceil(frameCount * outputSampleRate / sourceSampleRate), outputSampleRate);
    const buf = offlineCtx.createBuffer(1, frameCount, sourceSampleRate);
    buf.getChannelData(0).set(float32);
    const srcNode = offlineCtx.createBufferSource();
    srcNode.buffer = buf;
    srcNode.connect(offlineCtx.destination);
    srcNode.start(0);
    const rendered = await offlineCtx.startRendering();
    // Play the rendered buffer in main audioCtx
    const playSrc = audioCtx.createBufferSource();
    playSrc.buffer = rendered;
    playSrc.connect(audioCtx.destination);
    playSrc.start();
  } catch (e) {
    console.warn("playPCMBase64 error", e);
  }
}

// Start call: open WS, start mic capture, stream chunks
async function startCall() {
  const name = document.getElementById("name").value.trim();
  const dob = document.getElementById("dob").value;
  const tob = document.getElementById("tob").value;
  const pob = document.getElementById("pob").value.trim();
  const gender = document.getElementById("gender").value;
  const voice = document.getElementById("voice").value || "verse";

  if (!name || !dob || !tob || !pob || !gender) {
    setStatus("‚ö†Ô∏è ‡§ï‡•É‡§™‡§Ø‡§æ ‡§∏‡§≠‡•Ä ‡§µ‡§ø‡§µ‡§∞‡§£ ‡§≠‡§∞‡•á‡§Ç‡•§");
    return;
  }

  const wsUrl = (location.protocol === "https:" ? "wss://" : "ws://") + location.host + "/ws";
  ws = new WebSocket(wsUrl);
  setStatus("üõ∞ Connecting...");

  ws.onopen = async () => {
    setStatus("üîÆ Connected. Sending init...");
    ws.send(JSON.stringify({ type: "init", name, dob, tob, pob, gender, voice }));

    // small delay to let server create/open OpenAI WS
    await new Promise(r => setTimeout(r, 800));

    // start mic
    try {
      micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
    } catch (e) {
      setStatus("üîá Microphone permission denied.");
      console.error(e);
      return;
    }

    audioCtx = audioCtx || new (window.AudioContext || window.webkitAudioContext)();
    mediaSource = audioCtx.createMediaStreamSource(micStream);

    // ScriptProcessor is deprecated but widely supported ‚Äî fine for now
    scriptNode = audioCtx.createScriptProcessor(4096, 1, 1);
    mediaSource.connect(scriptNode);
    scriptNode.connect(audioCtx.destination);

    scriptNode.onaudioprocess = (e) => {
      try {
        if (!ws || ws.readyState !== WebSocket.OPEN) return;
        const float32 = e.inputBuffer.getChannelData(0);
        const b64 = floatToBase64(float32);
        ws.send(JSON.stringify({ type: "media", data: b64 }));
      } catch (err) {
        console.warn("onaudioprocess error", err);
      }
    };

    // Commit after a short delay, then periodic commits (2.5s)
    setTimeout(() => { if (ws && ws.readyState === WebSocket.OPEN) ws.send(JSON.stringify({ type: "media_commit" })); }, 1000);
    commitTimer = setInterval(() => {
      if (ws && ws.readyState === WebSocket.OPEN) ws.send(JSON.stringify({ type: "media_commit" }));
    }, 2500);

    connectBtn.style.display = "none";
    disconnectBtn.style.display = "inline-block";
    setStatus("üéô Live ‚Äî ‡§¨‡•ã‡§≤‡§ø‡§è...");
  };

  ws.onmessage = (evt) => {
    try {
      const msg = JSON.parse(evt.data);
      if (msg?.type === "output_audio_binary" && msg?.data) {
        // We assume source PCM sample rate is 16000 (OpenAI realtime often uses 16k)
        // Resampling code will handle mismatch to audioCtx.sampleRate.
        playPCMBase64(msg.data, 16000);
      } else {
        console.debug("server-msg", msg);
      }
    } catch (e) {
      console.warn("ws message parse error", e);
    }
  };

  ws.onerror = (err) => {
    console.error("WebSocket error:", err);
    setStatus("‚ùå Connection error.");
    stopCall();
  };

  ws.onclose = () => {
    setStatus("üì¥ Call ended.");
    stopCall();
  };
}

// Stop call and cleanup
function stopCall() {
  if (commitTimer) { clearInterval(commitTimer); commitTimer = null; }
  try { if (scriptNode) { scriptNode.disconnect(); scriptNode = null; } } catch {}
  try { if (mediaSource) { mediaSource.disconnect(); mediaSource = null; } } catch {}
  try { if (micStream) { micStream.getTracks().forEach(t => t.stop()); micStream = null; } } catch {}
  try { if (ws && ws.readyState === WebSocket.OPEN) ws.send(JSON.stringify({ type: "stop" })); } catch {}
  try { if (ws) ws.close(); } catch {}
  ws = null;
  connectBtn.style.display = "inline-block";
  disconnectBtn.style.display = "none";
}

connectBtn.addEventListener("click", startCall);
disconnectBtn.addEventListener("click", stopCall);
</script>
</body>
</html>
